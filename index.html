<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning">
    <meta property="og:title" content="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning" />
    <meta property="og:description" content="We propose an approach to incentivize mutlimodal reasoning capabilities" />
    <meta property="og:url" content="https://github.com/TIGER-AI-Lab/Pixel-Reasoner/" />
    <meta property="og:image" content="" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <title>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Alex Su<sup>*â™ â™¥</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://haozheh3.github.io/" style="text-decoration: none; color: inherit;">Haozhe Wang</a><sup>*â™¡â™¥</sup>,
                            </span>
                            <span class="author-block">
                                Weiming Ren<sup>â™¥</sup>,
                            </span>
                            <span class="author-block">
                                Fangzhen Lin<sup>â™¡</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Wenhu Chen</a><sup>â™¥</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <sup>â™ </sup>USTC, <sup>â™¡</sup>HKUST, <sup>â™¥</sup>University of Waterloo
                            </span>
                            <br>
			    <span class="author-block">*Equal contribution.</span>
			    <br>
                            <span class="author-block">Corresponding to:</span>
                            <span class="author-block"><a href="mailto:dlwlrma314516@gmail.com">dlwlrma314516@gmail.com</a>,</span>
                            <span class="author-block"><a href="mailto:jasper.whz@outlook.com ">jasper.whz@outlook.com </a>,</span>
                            <span class="author-block"><a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- GitHub link -->
                                <span class="link-block">
                                    <a href="https://github.com/TIGER-AI-Lab/Pixel-Reasoner" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2505.15966" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/TIGER-Lab/PixelReasoner-RL-v1" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        ðŸ¤—
                                      </span>
                                      <span>Models</span>
                                  </a>
                                </span>
                                <span class="link-block">
                                  <a href="https://huggingface.co/collections/TIGER-Lab/pixel-reasoner-682fe96ea946d10dda60d24e" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        ðŸ¤—
                                      </span>
                                      <span>Datasets</span>
                                  </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container" style="margin-bottom: 2vh;margin-top: -6vh;">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">ðŸ””News</h2>
              <p>We released models on huggingface. We are actively working on data and code release.</p>
            </div>
          </div>
          <!--/ Abstract. -->
      </div>
    </section>

    <section class="hero is-light is-small">
        <div class="hero-body has-text-centered">
          <h1 class="title is-1 acecoder">
            <span class="acecoder">Pixel-Reasoner</span>
          </h1>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h2 class="title is-3 has-text-centered">Motivation</h2>
              <div class="content has-text-justified">
                <p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. This reasoning paradigm is also employed by state-of-the-art VLMs, which first extracts relevant cues from mutlimodal inputs, and then perform reasoning purely in the textual format. 
                  Despite their success, this prevailing textual reasoning paradigm faces an inherent limitation: 
                  the lack of direct interaction with visual inputs -- such as drawing lines/marks, highlighting regions, or zooming in -- hinders the modelâ€™s ability to capture fine-grained visual details, including tiny objects, subtle spatial relationships, small embedded text, and nuanced actions in videos.
                These limitations lead us to pose a question:
                <p><b>Can VLMs perform reasoning steps more directly within the visual modality itself, leveraging computational visual manipulations as actions to guide reasoning?</b></p>
                <p>We introduce the concept of <b>pixel-space reasoning</b>, 
                  a novel paradigm where reasoning is not exclusively confined to verbalized format but actively incorporates operations applied directly to the visual inputs. 
                  These visual operations serve as integral steps within its reasoning chain, empowering the model to inspect, interrogate, and infer from visual evidence with enhanced fidelity. </p>
                <div class="columns is-centered mt-4 mb-4">
                  <div class="column is-10">
                    <figure class="image">
                        <img src="static/images/teaser.png" alt="VisualWebInstruct Pipeline">
                      <figcaption class="has-text-centered">Illustration of Pixel-Space Reasoning</figcaption>
                    </figure>
                  </div>
                </div>
                

              </div>
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h2 class="title is-3 has-text-centered">The Learning Trap of Cultivating Pixel-Space Reasoning</h2>
              <div class="content has-text-justified">
                <p>However, we identified a critical <b>learning trap</b> problem when cultivating this novel pixel-space reasoning capability. 
                  Existing VLMs typically suffers from a disparity in its capabilities: proficient textual reasoning versus nascent (zero-shot) pixel-space reasoning. This inherent imbalance creates a "learning trap" that impedes the development of pixel-space reasoning, stemming from two synergistic issues. 
                  Firstly, the model's initial limited mastery over visual operations frequently leads to failure or incorrect outputs, resulting in a higher incidence of negative feedback compared to text-mediated reasoning. 
                  Secondly, insufficient intrinsic motivations allows the model to purse the extrinsic correctness reward by simply ignoring the outcomes of visual operations or default to its stronger textual reasoning. 
                  This interplay fosters a detrimental cycle where initial failures discourage further attempts, leading to the premature abandonment of exploring and mastering visual operations. </p>
                <div class="columns is-centered mt-4 mb-4">
                  <div class="column is-10">
                    <figure class="image">
                        <img src="static/images/learningtrap.png" alt="VisualWebInstruct Pipeline">
                      <figcaption class="has-text-centered">Illustration of the Learning Trap Problem</figcaption>
                    </figure>
                  </div>
                </div>
                

              </div>
            </div>
          </div>
        </div>
      </section>
      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h2 class="title is-3 has-text-centered">How does it work?</h2>
              <div class="content has-text-justified">

                <p>We propose a two-phase training approach to cultivate pixel-space reasoning capabilities in VLMs. The first phase involves instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning.</p>
                

              </div>
            </div>
          </div>
        </div>
      </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-10">
            <h2 class="title is-3 has-text-centered">Instruction Tuning</h2>
            <div class="content has-text-justified">
              <p>Instruction tuning is the first phase of our training approach, where the model is familiarized with novel visual operations through synthesized reasoning trajectories.As shown in the datapipe, we first collect high-resolution images with rich information and videos as seed data. Then we localize the reference visual cues (bboxes or frame indexes) through the original annotation of the source data or using gpt's bbox to the generated questions.
                After that we employ a template-based synthesis approach. This template structures a pixel-space reasoning trajectory as a sequence: initial analysis of the entire visual input, followed by triggering specific visual operations to extract fine-grained details, subsequent analysis of these detailed visual cues, and ultimately arriving at the final answer. To synthesize a trajectory according to this template, we utilize the reference visual cue associated with each vision-language query. We first prompt GPT-4o to generate a textual description summarizing the entire visual input. Then, leveraging the reference visual cue, we prompt GPT-4o for a more detailed textual analysis focusing specifically on that cue. By composing these textual thinking segments and incorporating the visual operation targeting the reference visual cue, we obtain a single-pass reasoning trajectory that effectively interleaves textual reasoning with required visual operations.
                To enhance model's ability to react to unexpected visual outcome, we manually design some error cases and insert inaccurate visual operations before introducing the correct visual operations to form self-correction trajectories.


              </p>
              <div class="columns is-centered mt-4 mb-4">
                <div class="column is-10">
                  <figure class="image">
                      <img src="static/images/datapipeline.png" alt="VisualWebInstruct Pipeline" style="width: 100%; height: 100%;">
                    <figcaption class="has-text-centered">The Data synthesis pipeline</figcaption>
                  </figure>
                </div>
              </div>
            </div>
            
            
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-10">
            <h2 class="title is-3 has-text-centered">Curiosity-Driven RL</h2>
            <div class="content has-text-justified">
              <p>
                To overcome the learning trap where the model defaults to its stronger textual reasoning and neglects underdeveloped pixel-space reasoning, Pixel Reasoner introduces a curiosity-driven reward scheme. This scheme encourages active practice of visual operations by addressing two constraints: (1) the Rate of Pixel-space Reasoning (RaPR) should exceed a threshold H, and (2) the number of visual operations in a response should not exceed a bound N. These constraints are incorporated into training via Lagrangian relaxation, resulting in a modified reward function:
                \[
                r'(x, y) = r(x, y) + \alpha \cdot r_{\text{curiosity}}(x, y) + \beta \cdot r_{\text{penalty}}(y)
                \]
                where \( r_{\text{curiosity}} \) encourages exploration and \( r_{\text{penalty}} \) discourages overuse of visual operations. This reward formulation dynamically incentivizes the model to persist in learning pixel-space reasoning despite initial failure, leading to more robust visual understanding.
              </p>
              <div class="columns is-centered mt-4 mb-4">
                <div class="column is-5">
                  <figure class="image">
                    <img src="static/images/curiosity_toolcall_compare.png" alt="PDF Image 1" style="width: 100%; height: auto;">
                    <figcaption class="has-text-centered">RL Requires Incentives to Explore Pixel-space Reasoning. Without proper incentives, the policy learns to bypass the nascent pixel-space reasoning, resulting declining RaPR.</figcaption>
                  </figure>
                </div>
                <div class="column is-5">
                  <figure class="image">
                    <img src="static/images/curiosity_penalty.png" alt="PDF Image 2" style="width: 100%; height: auto;">
                    <figcaption class="has-text-centered">The Training Trend of our Curiosity-Driven Reward Scheme. We leverage curiosity bonus to encourages exploration and efficiency penalty to punish excessive visual operations.</figcaption>
                  </figure>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

	<!-- BibTeX citation -->
	<section class="section" id="BibTeX">
	    <div class="container is-max-desktop content">
	        <h2 class="title">Reference</h2>
	        If you find our work useful, please give us a free cite:
	        <pre><code>
			@article{pixel-reasoner,
			      title={Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning},
			      author = {Alex Su and Haozhe Wang and Weiming Ren and Fangzhen Lin and Wenhu Chen},
			      journal={arXiv preprint arXiv:2505.15966},
			      year={2024}
			}
	        </code></pre>
	    </div>
	</section>

	<footer class="footer">
	    <div class="container">
	        <div class="columns is-centered">
	            <div class="column is-8">
	                <div class="content has-text-centered">
	                    <p>
	                        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
	                    </p>
	                </div>
	            </div>
	        </div>
	    </div>
	</footer>

</body>
</html>
